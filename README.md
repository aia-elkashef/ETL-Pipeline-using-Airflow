# ETL-Pipeline-using-Airflow-IBM-Final-project

## This is the Final-project of ETL and Data Pipelines with Shell, Airflow and Kafka Course - IBM Data Engineering PROFESSIONAL CERTIFICATE on Coursera


The project aims to decongest the national highways by analyzing the road traffic data from different toll plazas. Each highway is operated by a different toll operator with different IT setups that use different file formats. 

#### The job is to create a data pipeline that collects data available in different formats and, consolidates it into a single file.

##### The main Tasks

Task 1: Define DAG arguments 

Task 2: Define the DAG 

Task 3: Create a task to download data 

Task 4: Create a task to extract data from csv file 

Task 5: Create a task to extract data from tsv file 

Task 6: Create a task to extract data from fixed width file 

Task 7: Create a task to consolidate data extracted from previous tasks 

Task 8: Transform the data 

Task 9: Define the task pipeline 

Task 10: Submit the DAG 

Task 11: Unpause the DAG 

Task 12: Monitor the DAG 

![alt text](https://github.com/aia-elkashef/ETL-Pipeline-using-Airflow/blob/main/dag_runs.png)

